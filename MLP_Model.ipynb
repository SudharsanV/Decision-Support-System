{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of MLP_Model.ipynb","version":"0.3.2","provenance":[{"file_id":"0B1g5uDHY2332VjU0RGFISXd1bW5ocjFrX01tNnpBOUpyVWZR","timestamp":1548577791727}]},"kernelspec":{"display_name":"conda_tensorflow_p36","language":"python","name":"conda_tensorflow_p36"}},"cells":[{"metadata":{"id":"veujLQ_fWbve","colab_type":"text"},"cell_type":"markdown","source":["Installing Required Packages"]},{"metadata":{"scrolled":true,"id":"8nybsJAiWbvj","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install python-docx\n","!pip install nltk\n","!pip install keras\n","!pip install gensim\n","!pip install tensorflow\n","!pip install sklearn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"a5JgdQB-Wbv4","colab_type":"text"},"cell_type":"markdown","source":["Reading Data From S3 Buckets"]},{"metadata":{"id":"OxRg8HomWbv6","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","import pandas as pd\n","import boto3\n","\n","bucket = \"projectworks\"\n","file_name = \"Cancer.csv\"\n","\n","s3 = boto3.client('s3') \n","\n","obj = s3.get_object(Bucket= bucket, Key= file_name) \n","\n","\n","initial_df = pd.read_csv(obj['Body'])\n","\n","data=initial_df[\"Description\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zxVfYgLGWbwA","colab_type":"code","colab":{}},"cell_type":"code","source":["voca=[]\n","text=''\n","temp_text=''\n","ct=0\n","l = list(data)\n","for i in l:\n","    text = text+\"\"+i\n","    voca.append(i)   \n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"yiJwri-lWbwF","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"UjNStp3sWbwK","colab_type":"code","colab":{}},"cell_type":"code","source":["def find_y(voca):\n","    c=len(voca)\n","    y=[1 for i in range(0,c)]\n","    for i in range(0,c) :\n","        Word=nltk.word_tokenize(voca[i])\n","        if 'II' in Word:\n","            y[i]=2\n","        if 'III' in Word:\n","            y[i]=3\n","        if 'IV' in Word:\n","            y[i]=4\n","        if 'V' in Word:\n","            y[i]=5\n","        if 'VI' in Word:\n","            y[i]=6    \n","        \n","    return y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sZi1mIsEWbwO","colab_type":"code","colab":{}},"cell_type":"code","source":["def bag_of_words(doc_words,doc_temp,vocabulary):    # boolean representation \n","    bool_vec = []\n","    temp_vec=[]\n","      \n","    for i in range(len(doc_words)):\n","        for j in range(len(vocabulary)):\n","            if vocabulary[j] in doc_words[i]:\n","               temp_vec.append(1)\n","            else:\n","                temp_vec.append(0)      \n","        \n","        bool_vec.append(temp_vec)       \n","        temp_vec=[]  \n","        \n","    return bool_vec"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3JsJsO4mWbwS","colab_type":"code","colab":{}},"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import string\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","\n","def clean_text(text):\n","    w_text = nltk.word_tokenize(text)\n","    \n","    punc_text=[]\n","    for i in w_text:\n","        if i not in string.punctuation:\n","            punc_text.append(i) \n","\n","    for i in punc_text:\n","        i=i.lower()\n","    \n","    stop_words=set(stopwords.words('english'))\n","\n","    non_stop_text=[]        \n","    for i in punc_text:\n","        if i not in stop_words:\n","            non_stop_text.append(i)\n","    \n","    unique_text = list(set(non_stop_text))\n","\n","    return unique_text\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lT3mfWMMWbwZ","colab_type":"code","colab":{}},"cell_type":"code","source":["def doc_words_cleaning(voca):       # cleans and preprocesses each document \n","    doc_words = []    \n","    for i in range(len(voca)):\n","        doc_words.append(clean_text(voca[i]))\n","    return doc_words\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b6gSKNTAWbwc","colab_type":"code","colab":{}},"cell_type":"code","source":["def bag_of_words(doc_words,doc_temp,vocabulary):    # boolean representation \n","    bool_vec = []\n","    temp_vec=[]\n","      \n","    for i in range(len(doc_words)):\n","        for j in range(len(vocabulary)):\n","            if vocabulary[j] in doc_words[i]:\n","               temp_vec.append(1)\n","            else:\n","                temp_vec.append(0)      \n","        \n","        bool_vec.append(temp_vec)       \n","        temp_vec=[]  \n","        \n","    return bool_vec"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o8E1QwC3Wbwg","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"MTjuJhD2Wbwk","colab_type":"code","colab":{}},"cell_type":"code","source":["def doc_term_freq(boo):\n","    doc_freq=[]\n","    for i in range(len(boo)):\n","        summ=0\n","        for j in range(len(boo[0])):\n","            if boo[i][j] == 1:\n","                summ=summ+1\n","        doc_freq.append(summ)\n","    return doc_freq\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IamZhGzsWbwo","colab_type":"code","colab":{}},"cell_type":"code","source":["# count of no of vocabulary words in each document\n","\n","def count_voca_words(bove):\n","    a=[]\n","    for i in range(len(bove)):\n","        a.append(bove[i].count(1))    \n","    return a\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rg8Sws2hWbwt","colab_type":"code","colab":{}},"cell_type":"code","source":["# term frequency [NAIVE] :-  \n","\n","def term_frequency_naive(doc_words,vocabulary):   \n","    tf=[]\n","    temp=[]\n","    for i in range(len(doc_words)):\n","        for j in vocabulary:\n","            temp.append(doc_words[i].count(j))\n","        tf.append(list(zip(temp,vocabulary)))\n","        temp=[]\n","    \n","    return tf            \n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wc6JUiBaWbwy","colab_type":"code","colab":{}},"cell_type":"code","source":["# tf - idf term frequency \n","\n","def tf(doc_words,vocabulary):\n","    tf=[]\n","    temp=[]\n","    for i in range(len(doc_words)):\n","        for j in vocabulary:\n","            temp.append((doc_words[i].count(j))/len(doc_words[i]))\n","        tf.append(temp)\n","        temp=[]\n","    return tf\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HqoJs2ZkWbw2","colab_type":"code","colab":{}},"cell_type":"code","source":["def idf(doc_words,vocabulary):\n","    import math\n","    doc_count = 0\n","    idf=[]\n","    for i in vocabulary:\n","        for j in range(len(doc_words)):\n","            if i in doc_words[j]:\n","                doc_count = doc_count + 1\n","        idf.append(math.log(len(doc_words)/(doc_count)))\n","        doc_count = 0 \n","    \n","    return idf\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YpZktXhDWbw6","colab_type":"code","colab":{}},"cell_type":"code","source":["def tf_idf_score_calc(tf,idf):\n","    tf_idf = []\n","    temp=[]\n","    for i in range(len(tf)):\n","        for j in range(len(idf)):\n","            if tf[i][j] !=0 and idf[j] !=0:\n","                temp.append(tf[i][j]*idf[j])\n","            else:\n","                temp.append(0)\n","        tf_idf.append(temp)\n","        temp=[]\n","    \n","    return tf_idf\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Gm6ox2UtWbxB","colab_type":"code","colab":{}},"cell_type":"code","source":["vocabulary = clean_text(text)\n","\n","doc_words = doc_words_cleaning(voca)\n","\n","bow = bag_of_words(doc_words,voca,vocabulary)\n","\n","y=find_y(voca)\n","\n","\n","doc_freq = count_voca_words(bow)   \n","\n","term_freq_naive = term_frequency_naive(doc_words,vocabulary)\n","\n","term_frequency = tf(doc_words,vocabulary)\n","\n","inverse_doc_freq = idf(doc_words,vocabulary)\n","\n","tf_idf_score = tf_idf_score_calc(term_frequency,inverse_doc_freq)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7wAy1-8_WbxG","colab_type":"text"},"cell_type":"markdown","source":["Building Neural Network Model"]},{"metadata":{"id":"lgH4spJMWbxI","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","from keras.utils import np_utils\n","encoder = LabelEncoder()\n","encoder.fit(y)\n","encoded_Y = encoder.transform(y)\n","# convert integers to dummy variables (i.e. one hot encoded)\n","dummy_y = np_utils.to_categorical(encoded_Y)\n","\n","import tensorflow as tfl\n","#Parameters\n","L_rate=0.1\n","epochs=100000000\n","\n","n_input=784\n","n_hidden=200\n","n_output=6\n","\n","X=tfl.placeholder(\"float\",[None,n_input])\n","Y=tfl.placeholder(\"float\",[None,n_output])\n","\n","w1=tfl.Variable(tfl.random_normal([n_input,n_hidden]))\n","w2=tfl.Variable(tfl.random_normal([n_hidden,n_output]))\n","\n","bias1=tfl.Variable(tfl.random_normal([n_hidden]))\n","bias2=tfl.Variable(tfl.random_normal([n_output]))\n","\n","def model(X):\n","  layer1=tfl.add(tfl.matmul(X,w1),bias1)\n","  layer1=tfl.nn.relu(layer1)\n","  output_layer=tfl.matmul(layer1,w2)+bias2\n","  return output_layer\n","  \n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KQKgvEjcWbxP","colab_type":"text"},"cell_type":"markdown","source":["Bilding The model"]},{"metadata":{"id":"0BPcXcaUWbxR","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"2gJHRn6vWbxT","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(bow, dummy_y)\n","#X_train, X_test, y_train, y_test = train_test_split(term_frequency, dummy_y)\n","#X_train, X_test, y_train, y_test = train_test_split(tf_idf_score, dummy_y)\n","\n","pred=model(X)\n","cost=tfl.reduce_mean(tfl.nn.softmax_cross_entropy_with_logits(logits=pred,labels=Y))\n","optimizer =tfl.train.AdamOptimizer(L_rate).minimize(cost)\n","init=tfl.global_variables_initializer()\n","\n","with tfl.Session() as sess:\n","  sess.run(init)\n","  for epochs in range(epochs):\n","    _.c=sess.run([optimizer,cost],feed_dict={X:X_train,Y:y_train})\n","    \n","  test_result = sess.run(pred,feed_dict= {X:X_train})\n","  y_pred=tfl.equal(tfl.argmax(test_result,1),tfl.argmax(y_train,1))\n","  accuracy=tfl.reduce_mean(tfl.cast(y_pred,\"float\"))\n","  \n","  print(\"Accuracy:\",accuracy.eval({X:X_test,Y:y_test}))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BqkHC5cOWbxX","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","#X_train, X_test, y_train, y_test = train_test_split(bow, dummy_y)\n","X_train, X_test, y_train, y_test = train_test_split(term_frequency, dummy_y)\n","#X_train, X_test, y_train, y_test = train_test_split(tf_idf_score, dummy_y)\n","\n","pred=model(X)\n","cost=tfl.reduce_mean(tfl.nn.softmax_cross_entropy_with_logits(logits=pred,labels=Y))\n","optimizer =tfl.train.AdamOptimizer(L_rate).minimize(cost)\n","init=tfl.global_variables_initializer()\n","\n","with tfl.Session() as sess:\n","  sess.run(init)\n","  for epochs in range(epochs):\n","    _.c=sess.run([optimizer,cost],feed_dict={X:X_train,Y:y_train})\n","    \n","  test_result = sess.run(pred,feed_dict= {X:X_train})\n","  y_pred=tfl.equal(tfl.argmax(test_result,1),tfl.argmax(y_train,1))\n","  accuracy=tfl.reduce_mean(tfl.cast(y_pred,\"float\"))\n","  \n","  print(\"Accuracy:\",accuracy.eval({X:X_test,Y:y_test}))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oRerqpFWWbxc","colab_type":"code","outputId":"35bf1f09-0a84-48fc-a023-9252e869f0de","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","#X_train, X_test, y_train, y_test = train_test_split(bow, dummy_y)\n","#X_train, X_test, y_train, y_test = train_test_split(term_frequency, dummy_y)\n","X_train, X_test, y_train, y_test = train_test_split(tf_idf_score, dummy_y)\n","\n","pred=model(X)\n","cost=tfl.reduce_mean(tfl.nn.softmax_cross_entropy_with_logits(logits=pred,labels=Y))\n","optimizer =tfl.train.AdamOptimizer(L_rate).minimize(cost)\n","init=tfl.global_variables_initializer()\n","\n","with tfl.Session() as sess:\n","  sess.run(init)\n","  for epochs in range(epochs):\n","    _.c=sess.run([optimizer,cost],feed_dict={X:X_train,Y:y_train})\n","    \n","  test_result = sess.run(pred,feed_dict= {X:X_train})\n","  y_pred=tfl.equal(tfl.argmax(test_result,1),tfl.argmax(y_train,1))\n","  accuracy=tfl.reduce_mean(tfl.cast(y_pred,\"float\"))\n","  \n","  print(\"Accuracy:\",accuracy.eval({X:X_test,Y:y_test}))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy: 0.28\n"],"name":"stdout"}]}]}